import pandas as pd
from tensorflow import keras
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity
from transformers import BertTokenizer, TFBertModel

# 1ï¸âƒ£ Parquet íŒŒì¼ ë¶ˆëŸ¬ì˜¤ê¸°
def load_embeddings(file_path):
    """Parquet íŒŒì¼ì—ì„œ ì„ë² ë”©ì„ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤."""
    try:
        df = pd.read_parquet(file_path)
        print(f"âœ… Parquet íŒŒì¼ì´ ì •ìƒì ìœ¼ë¡œ ë¶ˆëŸ¬ì™€ì¡ŒìŠµë‹ˆë‹¤. ë°ì´í„° ìˆ˜: {len(df)}")
    except FileNotFoundError:
        print(f"âŒ '{file_path}' íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. íŒŒì¼ ê²½ë¡œë¥¼ í™•ì¸í•˜ì„¸ìš”.")
        return None
    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
        return None
    return df

# 2ï¸âƒ£ BERT ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ
def load_bert_model():
    """BERT ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì €ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤."""
    print("ğŸ”„ BERT ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ë¡œë“œ ì¤‘...")
    tokenizer = BertTokenizer.from_pretrained("bert-base-multilingual-cased")
    bert_model = TFBertModel.from_pretrained("bert-base-multilingual-cased")
    print("âœ… BERT ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ë¡œë“œ ì™„ë£Œ!")
    return tokenizer, bert_model

# 3ï¸âƒ£ BERT ì„ë² ë”© ìƒì„± í•¨ìˆ˜
def get_bert_embedding(text, tokenizer, bert_model):
    """ì£¼ì–´ì§„ í…ìŠ¤íŠ¸ì˜ BERT ì„ë² ë”©ì„ ë°˜í™˜í•©ë‹ˆë‹¤. [CLS] ë²¡í„°ë§Œ ë°˜í™˜í•©ë‹ˆë‹¤."""
    if pd.isna(text) or text.strip() == '':
        return np.zeros((768,))
    inputs = tokenizer(text, return_tensors="tf", padding=True, truncation=True, max_length=128)
    outputs = bert_model(inputs)
    cls_embedding = outputs.last_hidden_state[:, 0, :].numpy()  # [CLS] ë²¡í„°
    return cls_embedding[0]  # (1, 768) -> (768,)

# 4ï¸âƒ£ ìœ ì‚¬ë„ ê³„ì‚° ë° ì±… ì¶”ì²œ
def recommend_books(user_input, df, tokenizer, bert_model, top_n=3):
    """ì…ë ¥ëœ ë¬¸ì¥ê³¼ ìœ ì‚¬í•œ ì±…ì„ ì¶”ì²œí•©ë‹ˆë‹¤."""
    print("ğŸ”„ ì…ë ¥ ë¬¸ì¥ì˜ ì„ë² ë”© ìƒì„± ì¤‘...")
    user_embedding = get_bert_embedding(user_input, tokenizer, bert_model).reshape(1, -1)

    # ëª¨ë“  ì±…ì˜ ì„ë² ë”©ì„ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
    book_embeddings = np.vstack(df['embedding'].values)

    # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
    similarities = cosine_similarity(user_embedding, book_embeddings)[0]

    # ìƒìœ„ top_nê°œì˜ ì±… ì¶”ì²œ
    df['similarity'] = similarities
    recommendations = df.sort_values(by='similarity', ascending=False).head(top_n)
    return recommendations[['TITLE_NM', 'AUTHR_NM', 'similarity']]

# 5ï¸âƒ£ ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜
def main():
    """ì „ì²´ ì¶”ì²œ ì‹œìŠ¤í…œì„ ì‹¤í–‰í•©ë‹ˆë‹¤."""
    # 1. Parquet íŒŒì¼ ê²½ë¡œ ì§€ì •
    file_path = '/filtered_book_embeddings.parquet' ##<- filtered_book_embeding.parquetíŒŒì¼ ì£¼ì†Œ ë„£ëŠ” ì½”ë“œ

    # 2. ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°
    df = load_embeddings(file_path)
    if df is None:
        return

    # 3. BERT ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ë¡œë“œ
    tokenizer, bert_model = load_bert_model()

    # 4. ì‚¬ìš©ì ì…ë ¥ ë°›ê¸°
    user_input = input("ğŸ“˜ ì–´ë–¤ ì±…ì„ ì°¾ê³  ê³„ì‹ ê°€ìš”? (ì˜ˆ: ë”°ëœ»í•œ ê°ë™ì„ ì£¼ëŠ” ì´ì•¼ê¸°, í¥ë¯¸ì§„ì§„í•œ ë¯¸ìŠ¤í„°ë¦¬ ë“±): ")
    
    # 5. ì±… ì¶”ì²œ
    recommendations = recommend_books(user_input, df, tokenizer, bert_model)

    # 6. ì¶”ì²œ ê²°ê³¼ ì¶œë ¥
    print("\nğŸ‰ ì¶”ì²œëœ ì±… ëª©ë¡:")
    print(recommendations)

# 6ï¸âƒ£ ë©”ì¸ í•¨ìˆ˜ ì‹¤í–‰
if __name__ == "__main__":
    main()
